{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0dyhr7k_7DS"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBiybVKy_2zi"
      },
      "source": [
        "# 3. Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtLP1aneFBZp"
      },
      "source": [
        "Currently, the model is experiencing the [checkerboard problem](https://distill.pub/2016/deconv-checkerboard/).\n",
        "<br/>\n",
        "<center><img src=\"images/shoe_maybe.png\" /></center>\n",
        "<br/>\n",
        "Thankfully, we have a few tricks up our generated T-shirt sleeve to resolve this and generally improve the performance of the model.\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "#### Learning Objectives\n",
        "\n",
        "The goals of this notebook are to:\n",
        "* Implement Group Normalization\n",
        "* Implement GELU\n",
        "* Implement Rearrange Pooling\n",
        "* Implement Sinusoidal Position Embeddings\n",
        "* Define a reverse diffusion function to emulate `p`\n",
        "* Attempt to generate articles of clothing (again)\n",
        "\n",
        "Like before, let's use fashionMIST to experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk1PoEXQOPxu",
        "outputId": "f8a7bcba-e2c2-41f5-9c4e-3977927754e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/GenAI/GenAI\n",
            "Collecting torchview\n",
            "  Downloading torchview-0.2.6-py3-none-any.whl.metadata (12 kB)\n",
            "Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: torchview\n",
            "Successfully installed torchview-0.2.6\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/GenAI/GenAI\"\n",
        "!pip install torchview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbBD2JzWM2P9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "from torchview import draw_graph\n",
        "import graphviz\n",
        "from IPython.display import Image\n",
        "\n",
        "# User defined libraries\n",
        "from utils import other_utils\n",
        "from utils import ddpm_utils\n",
        "\n",
        "IMG_SIZE = 16\n",
        "IMG_CH = 1\n",
        "BATCH_SIZE = 128\n",
        "data, dataloader = other_utils.load_transformed_fashionMNIST(IMG_SIZE, BATCH_SIZE)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DYJkF37M2QB"
      },
      "source": [
        "We have created a [ddpm_util.py](utils/ddpm_utils.py) with a `DDPM` class to group our diffusion functions. Let's use it to set up the same Beta schedule as what we used previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbaqDox8M2QC"
      },
      "outputs": [],
      "source": [
        "nrows = 10\n",
        "ncols = 15\n",
        "\n",
        "T = nrows * ncols\n",
        "B_start = 0.0001\n",
        "B_end = 0.02\n",
        "B = torch.linspace(B_start, B_end, T).to(device)\n",
        "ddpm = ddpm_utils.DDPM(B, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVVpRe-NM2QF"
      },
      "source": [
        "## 3.1 Group Normalization and GELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzyR8k7M2QG"
      },
      "source": [
        "The first improvement we will look at is optimizing our standard convolution process. We will be reusing this block many times throughout our neural network, so it is an important piece to get right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz15TBfGM2QI"
      },
      "source": [
        "### 3.1.1 Group Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCbM7ajAM2QL"
      },
      "source": [
        "[Batch Normalization](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338) converts the output of each kernel channel to a [z-score](https://www.nlm.nih.gov/oet/ed/stats/02-910.html). It does this by calculating the mean and standard deviation across a batch of inputs. This is ineffective if the batch size is small.\n",
        "\n",
        "On the other hand, [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf) normalizes the output of a group of kernels for each sample image, effectively \"grouping\" a set of features.\n",
        "\n",
        "<center><img src=\"images/groupnorm.png\" /></center>\n",
        "\n",
        "Considering color images have multiple color channels, this can have an interesting impact on the output colors of generated images. Try experimenting to see the effect!\n",
        "\n",
        "Learn more about normalization techniques in this [blog post](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8) by Aakash Bindal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPtPfoaVM2QM"
      },
      "source": [
        "### 3.1.2 GELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caV9feKHM2QN"
      },
      "source": [
        "[ReLU](https://www.kaggle.com/code/dansbecker/rectified-linear-units-relu-in-deep-learning) is a popular choice for an activation function because it is computationally quick and easy to calculate the gradient for. Unfortunately, it isn't perfect. When the bias term becomes largely negative, a ReLU neuron [\"dies\"](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks) because both its output and gradient are zero.\n",
        "\n",
        "At a slight cost in computational power, [GELU](https://arxiv.org/pdf/1606.08415.pdf) seeks to rectify the rectified linear unit by mimicking the shape of the ReLU function while avoiding a zero gradient.\n",
        "\n",
        "In this small example with FashionMNIST, it is unlikely we will see any dead neurons. However, the larger a model gets, the more likely it can face the dying ReLU phenomenon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k7itgbnwZ4y"
      },
      "outputs": [],
      "source": [
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.Conv2d(in_ch, out_ch, 3, 1, 1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx7ajI-1Q1T_"
      },
      "source": [
        "## 3.2 Rearrange pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlsbwna4M2QP"
      },
      "source": [
        "In the previous notebook, we used [Max Pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) to halve the size of our latent image, but is that the best technique? There are [many types of pooling layers](https://pytorch.org/docs/stable/nn.html#pooling-layers) including Min Pooling and Average Pooling. How about we let the neural network decide what is important.\n",
        "\n",
        "Enter the [einops](https://einops.rocks/1-einops-basics/) library and the [Rearrange](https://einops.rocks/api/rearrange/) layer. We can assign each layer a variable and use that to rearrange our values. Additionally, we can use parentheses `()` to identify a set of variables that are multiplied together.\n",
        "\n",
        "For example, in the code block below, we have:\n",
        "\n",
        "`Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)`\n",
        "\n",
        "* `b` is our batch dimension\n",
        "* `c` is our channel dimension\n",
        "* `h` is our height dimension\n",
        "* `w` is our width dimension\n",
        "\n",
        "We also have a `p1` and `p2` value that are both equal to `2`. The left portion of the equation before the arrow is saying \"split the height and width dimensions in half. The right portion of the equation after the arrow is saying \"stack the split dimensions along the channel dimension\".\n",
        "\n",
        "The code block below sets up a `test_image` to practice on. Try swapping `h` with `p1` on the left side of the arrow. What happens? How about when `w` and `p2` are swapped? What happens when `p1` is set to `3` instead of `2`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S8SgNB1M2QQ",
        "outputId": "295f62b7-5409-49d7-f64b-4b083de90c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[ 1,  2,  3,  4,  5,  6],\n",
            "          [ 7,  8,  9, 10, 11, 12],\n",
            "          [13, 14, 15, 16, 17, 18],\n",
            "          [19, 20, 21, 22, 23, 24],\n",
            "          [25, 26, 27, 28, 29, 30],\n",
            "          [31, 32, 33, 34, 35, 36]]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 1,  3,  5],\n",
              "          [13, 15, 17],\n",
              "          [25, 27, 29]],\n",
              "\n",
              "         [[ 2,  4,  6],\n",
              "          [14, 16, 18],\n",
              "          [26, 28, 30]],\n",
              "\n",
              "         [[ 7,  9, 11],\n",
              "          [19, 21, 23],\n",
              "          [31, 33, 35]],\n",
              "\n",
              "         [[ 8, 10, 12],\n",
              "          [20, 22, 24],\n",
              "          [32, 34, 36]]]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
        "\n",
        "test_image = [\n",
        "    [\n",
        "        [\n",
        "            [1, 2, 3, 4, 5, 6],\n",
        "            [7, 8, 9, 10, 11, 12],\n",
        "            [13, 14, 15, 16, 17, 18],\n",
        "            [19, 20, 21, 22, 23, 24],\n",
        "            [25, 26, 27, 28, 29, 30],\n",
        "            [31, 32, 33, 34, 35, 36],\n",
        "        ]\n",
        "    ]\n",
        "]\n",
        "test_image = torch.tensor(test_image)\n",
        "print(test_image)\n",
        "output = rearrange(test_image)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjaHDPi4M2QS"
      },
      "source": [
        "Next, we can pass this through our `GELUConvBlock` to let the neural network decide how it wants to weigh the values within our \"pool\". Notice the `4*in_chs` as a parameter of the `GELUConvBlock`? This is because the channel dimension is now p1 * p2 larger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIMeK6YRmIt"
      },
      "outputs": [],
      "source": [
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2)\n",
        "        self.conv = GELUConvBlock(4 * in_chs, in_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD7zbFIvM2QU"
      },
      "source": [
        "We now have the components to redefine our `DownBlock`s and `UpBlock`s. Multiple `GELUConvBlock`s have been added to help combat the checkerboard problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMexBq_Px8si"
      },
      "outputs": [],
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(DownBlock, self).__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mwCx92AM2QV"
      },
      "source": [
        "**TODO**: There's an input to the `UpBlock` that makes separates it from the `DownBlock`. What was it again?\n",
        "\n",
        "If needed, click the `...` below for the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXW3rwNfM2QW"
      },
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, FIXME):\n",
        "        x = FIXME\n",
        "        x = FIXME\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89mK0tBQM2QW",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super(UpBlock, self).__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(2 * in_chs, out_chs, 2, 2),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = torch.cat((x, skip), 1)\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHi36Uy2Rm7r"
      },
      "source": [
        "## 3.3 Time Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr0WBsnwM2QX"
      },
      "source": [
        "The better the model understands the timestep it is in for the reverse diffusion process, the better it will be able to correctly identify the added noise. In the previous notebook, we created an embedding for `t/T`. Can we help the model interpret this better?\n",
        "\n",
        "Before diffusion models, this was a problem that plagued natural language processing. For long dialogues, how can we capture where we are? The goal was to find a way to uniquely represent a large range of discrete numbers with a small number of continuous numbers. Using a single float is ineffective since the neural network will interpret timesteps as continuous rather than discrete. [Researchers](https://arxiv.org/pdf/1706.03762.pdf) ultimately settled on a sum of sines and cosines.\n",
        "\n",
        "For an excellent explanation for why this works and how this technique was likely developed, please refer to Jonathan Kernes' [Master Positional Encoding](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myGGt3V7_Y1O"
      },
      "outputs": [],
      "source": [
        "# https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
        "import math\n",
        "\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01iHx-JbM2QX"
      },
      "source": [
        "**TODO**: We will feed the output of the `SinusoidalPositionEmbedBlock` into our `EmbedBlock`. Thankfully, our `EmbedBlock` remains unchanged from before.\n",
        "\n",
        "It looks like the one below has been overrun with `FIXME`s. Can you remember how it was supposed to look?\n",
        "\n",
        "If needed, click the `...` below for the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoxUG8cDM2QX"
      },
      "outputs": [],
      "source": [
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, FIXME),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, FIXME),\n",
        "            nn.Unflatten(1, (FIXME, 1, 1))\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFRfF20fM2QX",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class EmbedBlock(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.Unflatten(1, (emb_dim, 1, 1))\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt1VNdd-M2QY"
      },
      "source": [
        "## 3.4 Residual Connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvtUQs_qM2QY"
      },
      "source": [
        "The last trick to eliminate the checkerboard problem is to add more residual or skip connections. We can create a `ResidualConvBlock` for our initial convolution. We could add residual connections in other places as well, such as within our \"DownBlocks\" and \"UpBlocks\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnKQ4tvwM2QY"
      },
      "outputs": [],
      "source": [
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GELUConvBlock(in_chs, out_chs, group_size)\n",
        "        self.conv2 = GELUConvBlock(out_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        out = x1 + x2\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Mq4YgnM2QY"
      },
      "source": [
        "Below is the updated model. Notice the change at the very last line? Another skip connection has been added from the output of our `ResidualConvBlock` to the final `self.out` block. This connection is surprisingly powerful, and of all the changes listed above, had the biggest influence on the checkerboard problem for this dataset.\n",
        "\n",
        "**TODO**: A couple of new variables have been added: `small_group_size` and `big_group_size` for group normalization. They are both dependent on the variable `group_base_size`. Set `group_base_size` to either `3`, `4`, `5`, `6`, or `7`. One of these values is correct and the rest will result in an error.\n",
        "\n",
        "**Hint**: The group sizes and `down_chs` are related.\n",
        "\n",
        "If needed, click the `...` below for the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMwOGI8M2QZ"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_chs = IMG_CH\n",
        "        down_chs = (64, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "        t_dim = 8\n",
        "        group_size_base = FIXME\n",
        "        small_group_size = 2 * group_size_base  # New\n",
        "        big_group_size = 8 * group_size_base  # New\n",
        "\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(img_chs, down_chs[0], small_group_size) # New\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size) # New\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size) # New\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_dim) # New\n",
        "        self.temb_1 = EmbedBlock(t_dim, up_chs[0])\n",
        "        self.temb_2 = EmbedBlock(t_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size) # New\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size) # New\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size) # New\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]), # New\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        down0 = self.down0(x)\n",
        "        down1 = self.down1(down0)\n",
        "        down2 = self.down2(down1)\n",
        "        latent_vec = self.to_vec(down2)\n",
        "\n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
        "        t = self.sinusoidaltime(t) # New\n",
        "        temb_1 = self.temb_1(t)\n",
        "        temb_2 = self.temb_2(t)\n",
        "\n",
        "        up0 = self.up0(latent_vec)\n",
        "        up1 = self.up1(up0+temb_1, down2)\n",
        "        up2 = self.up2(up1+temb_2, down1)\n",
        "        return self.out(torch.cat((up2, down0), 1)) # New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68fFH8CFM2Qa",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        img_chs = IMG_CH\n",
        "        down_chs = (64, 64, 128)\n",
        "        up_chs = down_chs[::-1]  # Reverse of the down channels\n",
        "        latent_image_size = IMG_SIZE // 4 # 2 ** (len(down_chs) - 1)\n",
        "        t_dim = 8\n",
        "        group_size_base = 4\n",
        "        small_group_size = 2 * group_size_base # New 2⁴ = 16\n",
        "        big_group_size = 8 * group_size_base  # New 8⁴ = 4096\n",
        "\n",
        "\n",
        "        # Inital convolution\n",
        "        self.down0 = ResidualConvBlock(img_chs, down_chs[0], small_group_size) # New\n",
        "\n",
        "        # Downsample\n",
        "        self.down1 = DownBlock(down_chs[0], down_chs[1], big_group_size) # New\n",
        "        self.down2 = DownBlock(down_chs[1], down_chs[2], big_group_size) # New\n",
        "        self.to_vec = nn.Sequential(nn.Flatten(), nn.GELU())\n",
        "\n",
        "        # Embeddings\n",
        "        self.dense_emb = nn.Sequential(\n",
        "            nn.Linear(down_chs[2]*latent_image_size**2, down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(down_chs[1], down_chs[2]*latent_image_size**2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.sinusoidaltime = SinusoidalPositionEmbedBlock(t_dim) # New\n",
        "        self.temb_1 = EmbedBlock(t_dim, up_chs[0])\n",
        "        self.temb_2 = EmbedBlock(t_dim, up_chs[1])\n",
        "\n",
        "        # Upsample\n",
        "        self.up0 = nn.Sequential(\n",
        "            nn.Unflatten(1, (up_chs[0], latent_image_size, latent_image_size)),\n",
        "            GELUConvBlock(up_chs[0], up_chs[0], big_group_size) # New\n",
        "        )\n",
        "        self.up1 = UpBlock(up_chs[0], up_chs[1], big_group_size) # New\n",
        "        self.up2 = UpBlock(up_chs[1], up_chs[2], big_group_size) # New\n",
        "\n",
        "        # Match output channels and one last concatenation\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * up_chs[-1], up_chs[-1], 3, 1, 1),\n",
        "            nn.GroupNorm(small_group_size, up_chs[-1]), # New\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(up_chs[-1], img_chs, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        down0 = self.down0(x)\n",
        "        print(\"down0:\", down0.size())\n",
        "        down1 = self.down1(down0)\n",
        "        print(\"down1:\", down1.size())\n",
        "        down2 = self.down2(down1)\n",
        "        print(\"down2:\", down2.size())\n",
        "        latent_vec = self.to_vec(down2)\n",
        "        print(\"latent_vec:\", latent_vec.size())\n",
        "\n",
        "        latent_vec = self.dense_emb(latent_vec)\n",
        "        print(\"dense_emb:\", latent_vec.size())\n",
        "        t = t.float() / T  # Convert from [0, T] to [0, 1]\n",
        "        t = self.sinusoidaltime(t) # New\n",
        "        print(\"sinusoidaltime:\", t.size())\n",
        "        temb_1 = self.temb_1(t)\n",
        "        print(\"temb_1:\", temb_1.size())\n",
        "        temb_2 = self.temb_2(t)\n",
        "        print(\"temb_2:\", temb_2.size())\n",
        "        up0 = self.up0(latent_vec)\n",
        "        print(\"up0:\", up0.size())\n",
        "        up1 = self.up1(up0+temb_1, down2)\n",
        "        print(\"up1:\", up1.size())\n",
        "        up2 = self.up2(up1+temb_2, down1)\n",
        "        print(\"up2:\", up2.size())\n",
        "        print(\"out:\", self.out(torch.cat((up2, down0), 1)).size())\n",
        "        return self.out(torch.cat((up2, down0), 1)) # New"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8buyYGqLOiNP",
        "outputId": "7123987a-3a70-4f92-f7fb-a26b4ddae90c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num params:  1979777\n",
            "down0: torch.Size([1, 64, 16, 16])\n",
            "down1: torch.Size([1, 64, 8, 8])\n",
            "down2: torch.Size([1, 128, 4, 4])\n",
            "latent_vec: torch.Size([1, 2048])\n",
            "dense_emb: torch.Size([1, 2048])\n",
            "sinusoidaltime: torch.Size([1, 8])\n",
            "temb_1: torch.Size([1, 128, 1, 1])\n",
            "temb_2: torch.Size([1, 64, 1, 1])\n",
            "up0: torch.Size([1, 128, 4, 4])\n",
            "up1: torch.Size([1, 64, 8, 8])\n",
            "up2: torch.Size([1, 64, 16, 16])\n",
            "out: torch.Size([1, 1, 16, 16])\n",
            "tensor([[[[-1.6122e-01,  1.3643e-01, -2.2218e-01, -1.2437e-01,  2.7172e-02,\n",
            "            1.7938e-01, -1.8569e-01,  1.4300e-04,  1.9116e-01,  3.6912e-02,\n",
            "            1.5431e-01,  3.6857e-01, -3.8823e-02, -1.1876e-01,  3.4781e-01,\n",
            "            4.9964e-02],\n",
            "          [-3.5325e-01, -1.1729e-01,  1.7069e-01, -3.8292e-02,  2.4864e-02,\n",
            "           -1.9231e-01,  3.6367e-01, -2.1130e-01, -3.5680e-01, -4.1446e-02,\n",
            "            2.7494e-01, -3.9636e-01, -3.6533e-02,  6.7134e-03,  3.2234e-02,\n",
            "            1.3268e-01],\n",
            "          [-1.7997e-01,  2.0827e-01,  1.7686e-01, -1.3979e-01,  6.4788e-01,\n",
            "            2.8186e-01, -1.8088e-01,  2.4544e-01,  4.4906e-01,  3.3251e-01,\n",
            "            4.9277e-01,  2.0615e-02, -1.7212e-01,  2.9486e-01,  7.0137e-02,\n",
            "            7.9588e-02],\n",
            "          [-3.1223e-01,  4.7748e-01, -2.7494e-01,  1.0788e-01, -3.3180e-01,\n",
            "            1.1563e-01,  8.0175e-03,  3.1720e-01,  1.2740e-01,  4.5720e-01,\n",
            "            8.4866e-02, -3.1311e-01, -1.3406e-01, -1.0429e-02, -1.7682e-01,\n",
            "            2.9063e-02],\n",
            "          [-1.0009e-01,  6.3791e-02, -1.3010e-01,  3.7869e-01,  5.9487e-02,\n",
            "            1.8985e-01, -2.6121e-01,  4.5947e-01, -2.6835e-01, -5.5478e-02,\n",
            "            5.7415e-01, -4.3773e-02, -5.1697e-01, -1.9547e-01,  6.9549e-01,\n",
            "            3.0731e-01],\n",
            "          [-5.1203e-01, -1.0157e-01, -1.1408e-01, -1.0522e-01, -8.1723e-02,\n",
            "            2.4732e-01, -1.3462e-01,  1.6687e-01,  1.1443e+00,  3.0881e-01,\n",
            "           -2.7300e-01, -5.9437e-02, -1.1037e-01, -7.9118e-02, -6.2934e-02,\n",
            "            2.3701e-01],\n",
            "          [-1.9903e-01, -2.9970e-01, -3.2993e-01,  7.0170e-02,  1.8192e-01,\n",
            "            4.0582e-01,  7.0377e-02,  3.0211e-02,  3.3361e-01, -9.8746e-02,\n",
            "           -4.7520e-01, -1.2733e-01, -3.9406e-01, -5.1801e-01,  3.3180e-01,\n",
            "           -2.6745e-04],\n",
            "          [-1.2008e-01, -2.2505e-01, -1.7787e-01, -1.6568e-01,  4.0688e-02,\n",
            "            2.7541e-01, -2.7216e-01,  4.7589e-01,  2.5798e-01,  3.1984e-01,\n",
            "            2.9934e-01, -1.4259e-01,  4.1637e-01,  1.0613e-01,  4.0510e-01,\n",
            "            1.6673e-01],\n",
            "          [-3.5150e-01, -3.0449e-01,  5.0668e-01,  3.4466e-01, -4.3998e-01,\n",
            "            6.0436e-01, -1.4881e-01, -4.8577e-01,  2.5880e-01,  1.2586e-01,\n",
            "            2.8091e-03,  8.7730e-02, -2.1095e-01,  1.5742e-01,  1.5938e-01,\n",
            "            4.5032e-01],\n",
            "          [-4.2580e-01,  2.1418e-01,  7.9069e-02,  6.6689e-02,  3.9077e-02,\n",
            "            2.4368e-01,  2.0907e-01, -6.9541e-01, -1.9155e-01,  4.4917e-01,\n",
            "           -5.3944e-01,  2.0499e-01,  1.8806e-01,  4.1666e-01, -1.2976e-01,\n",
            "            1.5219e-01],\n",
            "          [-5.5028e-03,  5.8692e-01,  2.5126e-01,  2.5871e-01,  3.3196e-01,\n",
            "           -2.7822e-01, -1.5125e-02,  6.7435e-02, -6.4027e-01,  2.2615e-01,\n",
            "           -2.3843e-01,  7.7671e-01,  2.1723e-01, -2.1294e-01,  3.2136e-01,\n",
            "            2.5422e-01],\n",
            "          [-1.8434e-01,  3.6747e-01,  4.6321e-02,  1.7095e-01,  5.9698e-01,\n",
            "           -1.5063e-01, -5.1578e-01, -1.0800e-01,  4.0251e-01, -1.0494e-01,\n",
            "           -2.9534e-02,  6.5419e-01,  6.1406e-01, -8.3671e-02, -1.2191e-02,\n",
            "            4.5243e-01],\n",
            "          [-1.2341e-01, -9.6298e-03,  3.3160e-01, -1.1446e-01,  3.0381e-01,\n",
            "           -4.9200e-01,  1.2764e-01,  9.8918e-02, -1.6905e-01,  1.7924e-01,\n",
            "            5.2141e-01,  1.9788e-01,  4.1285e-01,  3.3148e-01,  7.4932e-01,\n",
            "            6.9482e-01],\n",
            "          [-8.1839e-02, -8.1524e-02,  2.6338e-01, -1.2242e-01,  8.5742e-02,\n",
            "            1.4614e-01, -1.5691e-01,  2.5467e-01,  4.2697e-01,  4.6565e-01,\n",
            "            5.3217e-01, -1.2516e-02, -3.1509e-02,  3.0353e-01, -7.8819e-02,\n",
            "            4.4798e-01],\n",
            "          [-1.9419e-01,  8.0089e-02,  4.7762e-01,  6.4441e-02, -2.8224e-01,\n",
            "           -1.7412e-01,  3.2854e-02, -1.6049e-01,  2.3376e-01,  4.9436e-01,\n",
            "            4.2850e-01, -5.9356e-03,  3.7649e-01, -3.0552e-02, -1.2082e-01,\n",
            "            1.7125e-01],\n",
            "          [-4.4727e-02,  3.2283e-02, -8.7771e-02, -3.6696e-03, -1.5366e-01,\n",
            "            6.4895e-03, -1.4721e-02,  5.0873e-01,  2.2705e-01,  2.6518e-01,\n",
            "            3.9709e-01,  2.1448e-01, -9.8278e-02, -6.5682e-02,  9.1206e-02,\n",
            "            1.9560e-01]]]], grad_fn=<CompiledFunctionBackward>)\n"
          ]
        }
      ],
      "source": [
        "model = UNet()\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model = torch.compile(model.to(device))\n",
        "\n",
        "x = torch.randn(1, IMG_CH, IMG_SIZE, IMG_SIZE).to(device)\n",
        "t = torch.randint(0, T, (1,), device=device).float()\n",
        "\n",
        "print(model(x, t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0mtWiXeM2Qa"
      },
      "source": [
        "Finally, it's time to train the model. Let's see if all these changes made a difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BncOxHBrTDly"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "epochs = 5\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).float()\n",
        "        x = batch[0].to(device)\n",
        "        loss = ddpm.get_loss(model, x, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 1 == 0 and step % 100 == 0:\n",
        "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
        "            ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq_hWtK2cI2K"
      },
      "source": [
        "How about a closer look? Can you recognize a shoe, a purse, or a shirt?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-zupBmzcUuH"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "plt.figure(figsize=(8,8))\n",
        "ncols = 3 # Should evenly divide T\n",
        "for _ in range(10):\n",
        "    ddpm.sample_images(model, IMG_CH, IMG_SIZE, ncols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXe8LkSbM2Qb"
      },
      "source": [
        "## 3.5 Next"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYJewB97M2Qb"
      },
      "source": [
        "If you don't see a particular class such as a shoe or a shirt, try running the above code block again. Currently, our model does not accept category input, so the user can't define what kind of output they would like. Where's the fun in that?\n",
        "\n",
        "In the next notebook, we will finally add a way for users to control the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nn25syVM2Qb"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
